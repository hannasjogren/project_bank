{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:19:54.392579Z",
     "start_time": "2025-05-25T20:19:53.887260Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import pickle"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T20:19:54.980512Z",
     "start_time": "2025-05-25T20:19:54.598160Z"
    }
   },
   "cell_type": "code",
   "source": "transaktion_data = pd.read_csv(\"data/transactions.csv\", dtype=str)",
   "id": "68a5203029c01e27",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T20:19:55.034138Z",
     "start_time": "2025-05-25T20:19:55.018558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_file = \"data/sebank_customers_with_accounts.csv\"\n",
    "output_pickle = \"data/customers_cleaned.pkl\"\n",
    "log_file = \"logging_folder/duplicate_personnummer.log\"\n",
    "\n",
    "seen_personnummer = set()\n",
    "unique_rows = []\n",
    "duplicates = []\n",
    "\n",
    "with open(input_file, encoding='utf-8') as infile, \\\n",
    "     open(log_file, 'w', encoding='utf-8') as log:\n",
    "\n",
    "    reader = csv.DictReader(infile)\n",
    "\n",
    "    for row in reader:\n",
    "        personnummer = row.get('Personnummer')\n",
    "\n",
    "        if personnummer in seen_personnummer:\n",
    "            log.write(\",\".join(row.values()) + \"\\n\")\n",
    "            duplicates.append(row)\n",
    "        else:\n",
    "            seen_personnummer.add(personnummer)\n",
    "            unique_rows.append(row)\n",
    "\n",
    "with open(output_pickle, 'wb') as pkl_file:\n",
    "    pickle.dump(unique_rows, pkl_file)\n"
   ],
   "id": "1e20b3648858b9d0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T20:19:55.056935Z",
     "start_time": "2025-05-25T20:19:55.052850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_transaction = pd.DataFrame(transaktion_data)\n",
    "#df_transaction.dtypes"
   ],
   "id": "ea18b14b9f7d8464",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T20:19:55.255855Z",
     "start_time": "2025-05-25T20:19:55.078078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_transaction[\"amount\"] = df_transaction[\"amount\"].str.replace(\" \", \"\").str.replace(\",\", \".\")\n",
    "df_transaction[\"amount\"] = df_transaction[\"amount\"].astype(float)"
   ],
   "id": "617329064bc7dbf6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T20:19:55.285807Z",
     "start_time": "2025-05-25T20:19:55.281309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#df_transaction[\"currency\"].unique()\n",
    "#df_transaction[\"sender_country\"].unique()\n",
    "#df_transaction[\"receiver_country\"].unique()\n",
    "#df_transaction[\"notes\"].unique()"
   ],
   "id": "f0e72c0218b89175",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T20:19:55.322378Z",
     "start_time": "2025-05-25T20:19:55.308806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "expected_formats = [\n",
    "    \"%Y%m%d %H:%M:%S\",   # 20250125 04:48:00\n",
    "    \"%y-%m-%d %H:%M:%S\", # 25-04-09 12:12:00\n",
    "    \"%Y-%m-%d %H:%M\",    # 2025-01-18 16:14\n",
    "    \"%Y-%m-%d %H.%M\",    # 2025-01-30 23.30\n",
    "    \"%Y-%m-%d %H.%M:%S\"  # 2025-04-01 09.15:00\n",
    "]\n",
    "converted_data_with_ids = []\n",
    "invalid_data_with_ids = []\n",
    "\n",
    "try:\n",
    "    with open (\"logging_folder/timestamp_logs_with_ids\", \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            parts = line.split(',', 1)\n",
    "            if len(parts) < 2:\n",
    "                invalid_data_with_ids.append((None, f\"Formatfel i loggfilen: {line}\"))\n",
    "                continue\n",
    "\n",
    "            original_id = parts[0]\n",
    "            date_str = parts[1]\n",
    "\n",
    "            dt_obj = None\n",
    "            original_date_str_from_log = date_str\n",
    "\n",
    "            next_day_adjustment = False\n",
    "            if \" 24\" in date_str:\n",
    "                date_str = date_str.replace(\" 24\", \" 00\", 1)\n",
    "                next_day_adjustment = True\n",
    "\n",
    "            if dt_obj and next_day_adjustment:\n",
    "                dt_obj += pd.Timedelta(days=1)\n",
    "\n",
    "            if \".\" in date_str.split(\" \")[-1] and \":\" not in date_str.split(\" \")[-1]:\n",
    "                 time_part = date_str.split(\" \")[-1]\n",
    "                 if len(time_part.split('.')) == 2: # Ex: 12.30\n",
    "                     date_str = date_str.replace(time_part, time_part.replace('.', ':'))\n",
    "\n",
    "            for formats in expected_formats:\n",
    "                try:\n",
    "                    dt_obj = datetime.strptime(date_str, formats)\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            if dt_obj is None:\n",
    "                try:\n",
    "                    dt_obj = pd.to_datetime(date_str, errors=\"raise\")\n",
    "                except ValueError as e:\n",
    "                    invalid_data_with_ids.append((original_id, f\"{original_date_str_from_log} - pandas hanterar inte detta: {e}\"))\n",
    "                    continue\n",
    "\n",
    "            if dt_obj and next_day_adjustment:\n",
    "                dt_obj += pd.Timedelta(days=1)\n",
    "\n",
    "            if dt_obj:\n",
    "                converted_data_with_ids.append((original_id, dt_obj.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"I searched for the file: 'timestamp_logs_with_ids' but it remains hidden\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "print(\"\\nIts converted! (YYYY-MM-DD HH:MM:SS):\")\n",
    "for original_id, converted_date in converted_data_with_ids:\n",
    "    print(f\"ID: {original_id}, Datum: {converted_date}\")\n",
    "\n",
    "print(\"\\nUnknown error amongst id, file import or convertion error\")\n",
    "for original_id, error_msg in invalid_data_with_ids:\n",
    "    print(f\"ID: {original_id}, Fel: {error_msg}\")"
   ],
   "id": "40a2a4f98a338699",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Its converted! (YYYY-MM-DD HH:MM:SS):\n",
      "\n",
      "Unknown error amongst id, file import or convertion error\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T20:19:55.452621Z",
     "start_time": "2025-05-25T20:19:55.360084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if \"id\" in df_transaction.columns:\n",
    "    original_ids = df_transaction[\"id\"].copy()\n",
    "else:\n",
    "    original_ids = df_transaction.index.copy()\n",
    "\n",
    "original_timestamps = df_transaction[\"timestamp\"].copy()\n",
    "df_transaction[\"timestamp\"] = pd.to_datetime(df_transaction[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "df_valid_timestamps = df_transaction[df_transaction[\"timestamp\"].notna()]\n",
    "df_pending = df_transaction[df_transaction[\"timestamp\"].isna()]"
   ],
   "id": "5bd3d8a4e2c97a73",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T20:19:55.484101Z",
     "start_time": "2025-05-25T20:19:55.478102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pending_data_to_log = []\n",
    "for idx in df_pending.index:\n",
    "    item_id = original_ids[idx] if \"id\" in df_transaction.columns else idx\n",
    "    pending_data_to_log.append((item_id, original_timestamps[idx]))\n",
    "\n",
    "print(f\"Pending data to log: {pending_data_to_log}\")"
   ],
   "id": "8d7a6464a5677ce5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pending data to log: []\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T20:19:55.533099Z",
     "start_time": "2025-05-25T20:19:55.524849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not df_pending.empty:\n",
    "    with open(\"logging_folder/timestamp_logs_with_ids\", \"a\", encoding=\"utf-8\") as f:\n",
    "        for item_id, val in pending_data_to_log:\n",
    "            f.write(f\"{item_id},{val}\")\n",
    "    print(\"timestamp_logs_with_ids.txt har uppdaterats med ID:n.\")\n",
    "else:\n",
    "    print(\"Inga felaktiga datum att logga.\")\n"
   ],
   "id": "503318826b1ea672",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inga felaktiga datum att logga.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T20:19:55.580524Z",
     "start_time": "2025-05-25T20:19:55.570579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "id_to_converted_datetime = {item_id: dt_obj for item_id, dt_obj in converted_data_with_ids}\n",
    "\n",
    "for idx in df_pending.index:\n",
    "    current_id = original_ids[idx]\n",
    "\n",
    "    if current_id in id_to_converted_datetime:\n",
    "        df_transaction.loc[idx, \"timestamp\"] = id_to_converted_datetime[current_id]\n",
    "\n",
    "print(df_transaction[\"timestamp\"])"
   ],
   "id": "1455ca5ff5e1472",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       2025-01-08 03:17:00\n",
      "1       2025-01-02 19:34:00\n",
      "2       2025-01-12 20:08:00\n",
      "3       2025-02-08 06:24:00\n",
      "4       2025-03-01 18:51:00\n",
      "                ...        \n",
      "99995   2025-01-08 18:00:00\n",
      "99996   2025-03-20 10:00:00\n",
      "99997   2025-05-12 09:00:00\n",
      "99998   2025-03-05 15:00:00\n",
      "99999   2025-05-11 09:00:00\n",
      "Name: timestamp, Length: 100000, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T20:19:55.744164Z",
     "start_time": "2025-05-25T20:19:55.615722Z"
    }
   },
   "cell_type": "code",
   "source": "df_transaction.to_pickle(\"data/cleaned_transaction.pk1\")",
   "id": "f843732f2047d390",
   "outputs": [],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
